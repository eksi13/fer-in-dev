{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4243c-d15d-4343-a900-0cb3df057344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os \n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Tuple, Union\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d64927-95f7-4c5f-9de2-d265242a1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b49312-589b-48f8-a8e7-88d36b470857",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde693a-f5dc-4854-a0be-89f8291d04e1",
   "metadata": {},
   "source": [
    "#### Folder utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8779aef-bfaa-482b-a6eb-040e30fb2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ipynb_checkpoints(starting_dir):\n",
    "    for path in Path(starting_dir).rglob('.ipynb_checkpoints'):\n",
    "        print(\"Removing directory:\", path)\n",
    "        for file in path.glob('*'):\n",
    "            file.unlink()\n",
    "        path.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a3e2f-9d68-4643-9a87-063f66ccb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_and_create_paths(image_path, out_path):\n",
    "    resP = out_path / image_path.parent.name / image_path.name\n",
    "    if not resP.parent.exists():\n",
    "        resP.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return resP.parent\n",
    "\n",
    "def create_output_path(path, out_path):\n",
    "    return str(manage_and_create_paths(path, out_path)) + '/' + path.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f3d09-3521-43a4-857d-61ea2fec5d97",
   "metadata": {},
   "source": [
    "#### Image utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be75fdd-69a6-49bd-bbe4-694c4bb86c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(path, img):\n",
    "    cv2.imwrite(path, img)\n",
    "    \n",
    "def save_to_path(img, output_path, image_file_name, changes):\n",
    "    remove_file_extension(image_file_name)\n",
    "    image_out_path = f'{output_path}/{image_name}_{changes}.jpg'\n",
    "    save_image(img, image_out_path)\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def read_cv2_image(image_path):\n",
    "    return cv2.imread(str(image_path))\n",
    "\n",
    "def read_mp_cv2_image(image_path):\n",
    "    return mp.Image.create_from_file(str(image_path)), cv2.imread(str(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4b3b9-4b2a-4753-8876-c2170ef71544",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e2c02-3eb3-4221-acee-ebf83e2fb592",
   "metadata": {},
   "source": [
    "#### Rescaling pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0374ea-d642-4aea-b5e8-7b1f7aba4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    return img / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229c718-0132-4ca7-957f-2fd8ad8023bf",
   "metadata": {},
   "source": [
    "#### Histogram Equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32830b45-59d4-48d3-a225-36212e9bd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_eq(img):\n",
    "    return cv2.equalizeHist(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ae7c9-3abb-4599-bfad-5c9bd200b446",
   "metadata": {},
   "source": [
    "#### Normalization to zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c449e2-9fdc-45ea-ad73-b860f51dd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mean_var(img):\n",
    "    mean, std = cv2.meanStdDev(img)\n",
    "    return (img - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17913426-cb22-4060-bb56-bb26c8fc0039",
   "metadata": {},
   "source": [
    "#### Grayscale conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5725f5-ee7e-4fdf-9240-21b1b0a34ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85053980-f26d-4344-8e4d-ba6043d413c8",
   "metadata": {},
   "source": [
    "#### Viola Jones face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9120e0-1753-4f67-a8f7-694bcbb37ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns cropped image in cv2 format\n",
    "def crop_face(gray_img):\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = face_classifier.detectMultiScale(gray_img, scaleFactor=1.09, minNeighbors=5, minSize=(40, 40))\n",
    "    for f in faces:\n",
    "        x, y, w, h = [ v for v in f ]\n",
    "        cv2.rectangle(gray_img, (x,y), (x+w, y+h), (255,0,0), 3)\n",
    "    return gray_img[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84905c2b-679f-41eb-818b-fd0244dd0dcc",
   "metadata": {},
   "source": [
    "#### Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa09648-9aa3-422e-83a1-14f8dbc5e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes images based on given size and interpolation method\n",
    "def downsize_image(img, size):\n",
    "    return cv2.resize(img, dsize=size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def upsize_image(img, size):\n",
    "    return cv2.resize(img, dsize=size, interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172ab25-40f7-4edc-8ce9-12ab8a8081a0",
   "metadata": {},
   "source": [
    "### Mediapipe utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870dc05c-2626-404f-adb6-36b0ab945ec5",
   "metadata": {},
   "source": [
    "#### Mediapipe face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d2aeb-2a85-4ef6-bdb9-bf2552dfc25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "\n",
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "  def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
    "                                                      math.isclose(1, value))\n",
    "\n",
    "  if not (is_valid_normalized_value(normalized_x) and\n",
    "          is_valid_normalized_value(normalized_y)):\n",
    "    return None\n",
    "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "  return x_px, y_px\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image,\n",
    "    detection_result\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  annotated_image = image.copy()\n",
    "  height, width, _ = image.shape\n",
    "\n",
    "  for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in detection.keypoints:\n",
    "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
    "                                                     width, height)\n",
    "      color, thickness, radius = (0, 255, 0), 2, 2\n",
    "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    category_name = category.category_name\n",
    "    category_name = '' if category_name is None else category_name\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = category_name + ' (' + str(probability) + ')'\n",
    "    text_location = (MARGIN + bbox.origin_x,\n",
    "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb573a3d-18b2-4c08-839a-c34bd5d776bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_faces(detection_results, img_cv2, out_path):\n",
    "    for d in detection_results.detections:  \n",
    "        bbox = d.bounding_box\n",
    "        origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "        cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "        cv2.imwrite(out_path, cropped_img)\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c04ff9f-30bd-45f6-8eef-d9d61451fdbf",
   "metadata": {},
   "source": [
    "#### Mediapipe face mesh utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a62c5-f0f9-4a10-951a-14a3fd69ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title('Face Blendshapes')\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b2b7a-e1f5-49fb-bc47-0e281614a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_landmarks(c_df, img_cv2, idx):\n",
    "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "    detection_result = detector_mesh.detect(rgb_frame)\n",
    "    df = pd.DataFrame([(idx, p.name, index, point.x, point.y, point.z) for index, point in enumerate(detection_result.face_landmarks[0])], columns=['image_idx', 'image_name', 'landmark_idx', 'x', 'y', 'z'])\n",
    "    c_df = c_df.append(df)\n",
    "    return c_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060db70-7dda-4dc7-a903-38c39abf6ee3",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdca529-730e-46c0-bb0d-de498fe27179",
   "metadata": {},
   "source": [
    "#### Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56315197-9100-4020-8b43-8131ca2cefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nimh_in = Path('./data/in/NIMH-CHEFS')\n",
    "fer_in = Path('./data/in/FER-2013')\n",
    "\n",
    "nimh_out = Path('./data/out/NIMH-CHEFS')\n",
    "fer_out = Path('./data/out/FER-2013')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92ad38-8ebd-4ab5-9d7f-5379330371e3",
   "metadata": {},
   "source": [
    "#### Resizing size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9983b4d-7936-4e07-9d19-1734fd078706",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6fb82-9e04-4dc8-8b55-358d4d0e4609",
   "metadata": {},
   "source": [
    "#### Model path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1edfc-391c-4fe5-8dd5-17eefca8c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_detect = './models/blaze_face_short_range.tflite'\n",
    "model_path_mesh = './models/face_landmarker.task'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e66625-e328-45cb-8a82-2a61795a44ea",
   "metadata": {},
   "source": [
    "#### Model options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53deb2d0-3230-4e69-b0e4-a9513f313ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options_detect = python.BaseOptions(model_asset_path=model_path_detect)\n",
    "options_detect = vision.FaceDetectorOptions(base_options=base_options_detect)\n",
    "detector_detect = vision.FaceDetector.create_from_options(options_detect)\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b31f0-4a42-4689-b58c-620a5f6b9918",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be8254-e7b9-46a7-a885-c4089df964e5",
   "metadata": {},
   "source": [
    "#### Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f2f3e-831b-43fe-8fad-5c42f572dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in (nimh_in.rglob('*.jpg')):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(p)\n",
    "    detection_results = detector_detect.detect(img_mp)\n",
    "    cropped_img = crop_faces(detection_results, img_cv2, path)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fc7b2-9214-4d96-970c-2503304ecd9c",
   "metadata": {},
   "source": [
    "#### Face mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58774938-2125-49de-b841-85a898c9f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in (nimh_out.rglob('*.jpg')):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(p)\n",
    "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c9f70-4970-48f7-b18f-f00db90e41e3",
   "metadata": {},
   "source": [
    "#### Process detection result, visualize detected face with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff9c27-6562-4709-87fd-7f9f35fa9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_copy = np.copy(img_mp.numpy_view())\n",
    "annotated_image = visualize(image_copy, detection_results)\n",
    "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "show_image(rgb_annotated_image)\n",
    "show_image(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5a836-6774-48e8-b876-b01101fd19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in (nimh_out.rglob('*.jpg')):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img = read_cv2_image(p)\n",
    "    \n",
    "    img = downsize_image(img, size)\n",
    "\n",
    "    img = convert_to_grayscale(img)\n",
    "    img = histogram_eq(img) # needs grayscale to work\n",
    "    \n",
    "    # TODO: fix these both\n",
    "    #img = normalize_mean_var(img) # results in black image?\n",
    "    #img = normalize(img) # results in black image?\n",
    "    \n",
    "    show_image(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9d2b3-8a9f-4c7c-bfb6-42968661efc7",
   "metadata": {},
   "source": [
    "#### Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32032d3-650b-4dbd-9545-520dd843a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect face landmarks from the input image\n",
    "detection_result = detector_mesh.detect(rgb_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0db907-890f-44ff-81fa-6cb0ca0649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0562d3-1c28-4b65-8902-aa7439d55ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_image = draw_landmarks_on_image(rgb_frame.numpy_view(), detection_result)\n",
    "res_img = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(res_img, cv2.COLOR_RGB2BGR))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d1f1c-eb66-4b2b-a56f-927f22913972",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ded57-401a-4cc5-8c9c-db117e29918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ipynb_checkpoints('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f822dab-b6e7-466d-87d5-f02bbdd107a2",
   "metadata": {},
   "source": [
    "## First, run face detection and cropping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d6623-eaa6-4860-ab9f-5ce15996027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm(list(nimh_in.rglob('*.jpg'))):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(p)\n",
    "    img_cv2 = convert_to_grayscale(img_cv2)\n",
    "    detection_results = detector_1.detect(img_mp)\n",
    "    crop_faces(detection_results, img_cv2, path)\n",
    "    display('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac59a15-a1a6-47ce-bbd8-f504f1a664ab",
   "metadata": {},
   "source": [
    "## Run the rest of the preprocessing as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c61a8-9d25-471d-8125-7bc62da0d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in (nimh_out.rglob('*.jpg')):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img = read_cv2_image(p)\n",
    "    \n",
    "    img = downsize_image(img, size)\n",
    "\n",
    "    img = convert_to_grayscale(img)\n",
    "    img = histogram_eq(img) # needs grayscale to work\n",
    "    \n",
    "    # TODO: fix these both\n",
    "    #img = normalize_mean_var(img) # results in black image?\n",
    "    #img = normalize(img) # results in black image?\n",
    "\n",
    "    save_image(path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018acc9c-de3e-4d4f-8162-6d225f76ba8e",
   "metadata": {},
   "source": [
    "## Run the feature extarction (facial landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43cb00-0ec3-4b1f-a5a9-3ea60f1b2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for p in (nimh_out.rglob('*.jpg')):\n",
    "    path = create_output_path(p, nimh_out)\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(p)\n",
    "    c_df = extract_and_save_landmarks(c_df, img_cv2, i)\n",
    "    i = i + 1  \n",
    "\n",
    "c_df.to_csv('./landmarks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
