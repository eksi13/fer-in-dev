{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pickle\n",
    "    import random\n",
    "    from pathlib import Path\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from tqdm import tqdm\n",
    "    from collections import Counter\n",
    "\n",
    "    from skimage.feature import hog\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "    import mediapipe as mp\n",
    "    from mediapipe import Image\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "    import mediapipe as mp\n",
    "    from mediapipe import Image\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    from torchvision import transforms, datasets, models\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "except:\n",
    "    %pip install mediapipe\n",
    "    %pip install torch\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe import Image\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import mediapipe as mp\n",
    "from mediapipe import Image\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from PIL import Image\n",
    "\n",
    "    \n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Feature extraction method\n",
    "FEATURE = 'pixel'\n",
    "\n",
    "# Parameters for Histogram of Oriented Gradients (HOG) feature extraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size, transform=None):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "\n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path / TRAIN).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            img, label = self.__getitem__(i)\n",
    "            if img.ndim == 3:\n",
    "                img = img.squeeze(0)\n",
    "            img = img.numpy() if isinstance(img, torch.Tensor) else np.array(img)\n",
    "\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        return cv2.imread(str(img_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, dataset, feature='pixels', img_size=64, n_splits_values = [3, 5, 10],\n",
    "                 model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):   \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.img_size = img_size\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.n_split_values = n_splits_values\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.best_estimator = None\n",
    "\n",
    "        # mediapipe\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=False, \n",
    "                                                         output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    def extract_features(self):\n",
    "        print(f\"[INFO] Extracting {self.feature} vectors ...\")\n",
    "\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            \n",
    "            img, label = self.dataset[idx]\n",
    "            img_cv2 = self.dataset.get_cv2_img(idx)\n",
    "\n",
    "            if self.feature == 'landmark':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif self.feature == 'pixel':\n",
    "                        \n",
    "                labels.append(label)\n",
    "                data.append(np.array(img_cv2).flatten())\n",
    "\n",
    "            elif self.feature == 'hog':\n",
    "                \n",
    "                orientations = self.hog_args.get('orientations', None)\n",
    "                image_shape = self.hog_args.get('image_shape', None)\n",
    "                pixels_per_cell = self.hog_args.get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args.get('cells_per_block', None)\n",
    "\n",
    "                if orientations is None or image_shape is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, image_shape, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "\n",
    "            elif self.feature == 'blendshape':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {self.feature}! Should be one of: [blendshape, pixel, landmark, hog]\")\n",
    "\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        self.data, self.labels = self.extract_features()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.2, shuffle=True, stratify=self.labels, random_state=42)\n",
    "\n",
    "        classifier = SVC()\n",
    "        parameters = {\"gamma\" : [0.1, 0.01, 0.001], 'C' : [1, 10, 100, 1000]}\n",
    "\n",
    "        grid_search = GridSearchCV(classifier, parameters)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        self.best_estimator = grid_search.best_estimator_\n",
    "        print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "        pickle.dump(self.best_estimator, open(str(MODEL_PATH / DATA / f'{self.feature}_model.p'), 'wb'))\n",
    "\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=True)\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=False)\n",
    "\n",
    "\n",
    "    def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "        if train:\n",
    "            dataset_type = \"Train\"\n",
    "            data, labels = x_train, y_train\n",
    "        else:\n",
    "            dataset_type = \"Test\"\n",
    "            data, labels = x_test, y_test\n",
    "\n",
    "        y_prediction = clf.predict(data)\n",
    "        clf_report = classification_report(labels, y_prediction)\n",
    "        accuracy = accuracy_score(labels, y_prediction) * 100\n",
    "        confusion_mat = confusion_matrix(labels, y_prediction)\n",
    "\n",
    "        print(f\"{dataset_type} Result:\\n{'=' * 50}\")\n",
    "        print(f\"Accuracy Score: {accuracy:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n{confusion_mat}\\n\")\n",
    "\n",
    "    def k_fold(self):\n",
    "        print(f\"[INFO] Evaluating mode: {self.feature}\")\n",
    "\n",
    "        for n_splits in self.n_splits_values:\n",
    "            cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "            scores = cross_val_score(self.best_estimator, self.data, self.labels, scoring='accuracy', cv=cv, n_jobs=2)\n",
    "            print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "nimhchefs = Dataset(data_path=CURRENT_PATH, img_size=64, transform=feature_extraction_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVMClassifier(dataset=nimhchefs, feature=FEATURE, img_size=64, **hog_params)\n",
    "print(f\"[INFO] Training SVM ...\")\n",
    "svm_classifier.train()\n",
    "print(f\"[INFO] k-fold ...\")\n",
    "svm_classifier.k_fold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
