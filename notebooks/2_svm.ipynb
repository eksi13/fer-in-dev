{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Feature extraction method\n",
    "FEATURE = 'pixel'\n",
    "\n",
    "# Parameters for Histogram of Oriented Gradients (HOG) feature extraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size=64, feature='pixels', transform=None, model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # mediapipe\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=False, \n",
    "                                                         output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path / TRAIN).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            img, label = self.__getitem__(i)\n",
    "            if img.ndim == 3:\n",
    "                img = img.squeeze(0)\n",
    "            img = img.numpy() if isinstance(img, torch.Tensor) else np.array(img)\n",
    "\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_cv2 = self.get_cv2_img(idx)\n",
    "        plt.imshow(img_cv2, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        return cv2.imread(str(img_path))\n",
    "    \n",
    "\n",
    "    def extract_features(self):\n",
    "        print(f\"[INFO] Extracting {self.feature} vectors ...\")\n",
    "\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            \n",
    "            img, label = self.dataset[idx]\n",
    "            img_cv2 = self.dataset.get_cv2_img(idx)\n",
    "\n",
    "            if self.feature == 'landmarks':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif self.feature == 'pixels':\n",
    "                        \n",
    "                labels.append(label)\n",
    "                img_array = np.array(img)\n",
    "                data.append(img_array)\n",
    "\n",
    "\n",
    "            elif self.feature == 'hog':\n",
    "                \n",
    "                orientations = self.hog_args.get('orientations', None)\n",
    "                image_shape = self.hog_args.get('image_shape', None)\n",
    "                pixels_per_cell = self.hog_args.get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args.get('cells_per_block', None)\n",
    "\n",
    "                if orientations is None or image_shape is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, image_shape, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "\n",
    "            elif self.feature == 'blendshapes':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {self.feature}! Should be one of: [blendshape, pixel, landmark, hog]\")\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "        return self.data, self.labels\n",
    "\n",
    "\n",
    "    def to_df(self, to_csv=False):\n",
    "        df = pd.DataFrame()\n",
    "        df[FEATURE] = [row for row in self.data.reshape(len(self.dataset), -1)]\n",
    "        df['emotion'] = [value.item() for value in self.labels.reshape(-1, 1)]\n",
    "        if to_csv:\n",
    "            df.to_csv('features.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, dataset, feature='pixels', n_splits_values = [3, 5, 10], **kwargs):   \n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.n_split_values = n_splits_values\n",
    "        self.best_estimator = None\n",
    "        self.dt_params = kwargs\n",
    "\n",
    "    def train(self):\n",
    "        self.data, self.labels = self.extract_features()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.2, shuffle=True, stratify=self.labels, random_state=42)\n",
    "\n",
    "        classifier = SVC()\n",
    "        parameters = {\"gamma\" : [0.1, 0.01, 0.001], 'C' : [1, 10, 100, 1000]}\n",
    "\n",
    "        grid_search = GridSearchCV(classifier, parameters)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        self.best_estimator = grid_search.best_estimator_\n",
    "        print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "        pickle.dump(self.best_estimator, open(str(MODEL_PATH / DATA / f'{self.feature}_model.p'), 'wb'))\n",
    "\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=True)\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=False)\n",
    "\n",
    "\n",
    "    def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "        if train:\n",
    "            dataset_type = \"Train\"\n",
    "            data, labels = x_train, y_train\n",
    "        else:\n",
    "            dataset_type = \"Test\"\n",
    "            data, labels = x_test, y_test\n",
    "\n",
    "        y_prediction = clf.predict(data)\n",
    "        clf_report = classification_report(labels, y_prediction)\n",
    "        accuracy = accuracy_score(labels, y_prediction) * 100\n",
    "        confusion_mat = confusion_matrix(labels, y_prediction)\n",
    "\n",
    "        print(f\"{dataset_type} Result:\\n{'=' * 50}\")\n",
    "        print(f\"Accuracy Score: {accuracy:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n{confusion_mat}\\n\")\n",
    "\n",
    "\n",
    "    def k_fold(self):\n",
    "        print(f\"[INFO] Evaluating mode: {self.feature}\")\n",
    "\n",
    "        for n_splits in self.n_splits_values:\n",
    "            cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "            scores = cross_val_score(self.best_estimator, self.data, self.labels, scoring='accuracy', cv=cv, n_jobs=2)\n",
    "            print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "nimhchefs = Dataset(data_path=CURRENT_PATH, img_size=64, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVMClassifier(dataset=nimhchefs, feature=FEATURE, img_size=64, **hog_params)\n",
    "\n",
    "svm_classifier.train()\n",
    "\n",
    "svm_classifier.k_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_detect = '/home/jovyan/work/models/blaze_face_short_range.tflite'\n",
    "model_path_mesh = '/home/jovyan/work/models/face_landmarker.task'\n",
    "\n",
    "base_options_detect = python.BaseOptions(model_asset_path=model_path_detect)\n",
    "options_detect = vision.FaceDetectorOptions(base_options=base_options_detect)\n",
    "detector_detect = vision.FaceDetector.create_from_options(options_detect)\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, sys, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Union\n",
    "import pandas as pd\n",
    "\n",
    "# mediapipe\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_faces(img_cv2, detection_results, out_path):\n",
    "    for d in detection_results.detections:  \n",
    "        bbox = d.bounding_box\n",
    "        origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "        cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "        #show_image(cropped_img)      \n",
    "        cv2.imwrite(str(out_path), cropped_img)\n",
    "        return cropped_img\n",
    "    \n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title('Face Blendshapes')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# extract landmarks to csv\n",
    "def extract_and_save_landmarks(c_df, img_cv2, idx):\n",
    "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "    detection_result = detector_mesh.detect(rgb_frame)\n",
    "    df = pd.DataFrame([(idx, p.name, index, point.x, point.y, point.z) for index, point in enumerate(detection_result.face_landmarks[0])], columns=['image_idx', 'image_name', 'landmark_idx', 'x', 'y', 'z'])\n",
    "    c_df = c_df.append(df)\n",
    "    return c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
