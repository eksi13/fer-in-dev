{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.14-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.7 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.11/site-packages (from mediapipe) (23.2.0)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Downloading jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Downloading jaxlib-0.4.29-cp311-cp311-manylinux2014_aarch64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from mediapipe) (3.8.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Downloading opencv_contrib_python-4.10.0.82-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/conda/lib/python3.11/site-packages (from mediapipe) (4.25.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.4.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Collecting ml-dtypes>=0.4.0 (from jax->mediapipe)\n",
      "  Downloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from jax->mediapipe)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.11/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.9.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading mediapipe-0.10.14-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (33.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading sounddevice-0.4.7-py3-none-any.whl (32 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m367.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.29-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.29-cp311-cp311-manylinux2014_aarch64.whl (65.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.10.0.82-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (47.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m962.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: flatbuffers, opt-einsum, opencv-contrib-python, ml-dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "Successfully installed absl-py-2.1.0 flatbuffers-24.3.25 jax-0.4.29 jaxlib-0.4.29 mediapipe-0.10.14 ml-dtypes-0.4.0 opencv-contrib-python-4.10.0.82 opt-einsum-3.3.0 sounddevice-0.4.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "import successful\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Feature extraction method\n",
    "FEATURE = 'pixel'\n",
    "\n",
    "# Parameters for Histogram of Oriented Gradients (HOG) feature extraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size=64, feature='pixels', transform=None, model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # mediapipe\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=False, \n",
    "                                                         output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path / TRAIN).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            _, label = self.__getitem__(i)\n",
    "            img_cv2 = self.get_cv2_img(i)\n",
    "\n",
    "            ax.imshow(img_cv2, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_cv2 = self.get_cv2_img(idx)\n",
    "        plt.imshow(img_cv2, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        return cv2.imread(str(img_path))\n",
    "    \n",
    "\n",
    "    def extract_features(self):\n",
    "        print(f\"[INFO] Extracting {self.feature} vectors ...\")\n",
    "\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            \n",
    "            img, label = self.dataset[idx]\n",
    "            img_cv2 = self.dataset.get_cv2_img(idx)\n",
    "\n",
    "            if self.feature == 'landmarks':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif self.feature == 'pixels':\n",
    "                        \n",
    "                labels.append(label)\n",
    "                img_array = np.array(img)\n",
    "                data.append(img_array)\n",
    "\n",
    "\n",
    "            elif self.feature == 'hog':\n",
    "                \n",
    "                orientations = self.hog_args.get('orientations', None)\n",
    "                image_shape = self.hog_args.get('image_shape', None)\n",
    "                pixels_per_cell = self.hog_args.get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args.get('cells_per_block', None)\n",
    "\n",
    "                if orientations is None or image_shape is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, image_shape, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "\n",
    "            elif self.feature == 'blendshapes':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {self.feature}! Should be one of: [blendshape, pixel, landmark, hog]\")\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "        return self.data, self.labels\n",
    "\n",
    "\n",
    "    def to_df(self, to_csv=False):\n",
    "        df = pd.DataFrame()\n",
    "        df[FEATURE] = [row for row in self.data.reshape(len(self.dataset), -1)]\n",
    "        df['emotion'] = [value.item() for value in self.labels.reshape(-1, 1)]\n",
    "        if to_csv:\n",
    "            df.to_csv('features.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, dataset, feature='pixels', n_splits_values = [3, 5, 10], **kwargs):   \n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.n_split_values = n_splits_values\n",
    "        self.best_estimator = None\n",
    "        self.dt_params = kwargs\n",
    "\n",
    "    def train(self):\n",
    "        self.data, self.labels = self.extract_features()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.2, shuffle=True, stratify=self.labels, random_state=42)\n",
    "\n",
    "        classifier = SVC()\n",
    "        parameters = {\"gamma\" : [0.1, 0.01, 0.001], 'C' : [1, 10, 100, 1000]}\n",
    "\n",
    "        grid_search = GridSearchCV(classifier, parameters)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        self.best_estimator = grid_search.best_estimator_\n",
    "        print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "        pickle.dump(self.best_estimator, open(str(MODEL_PATH / DATA / f'{self.feature}_model.p'), 'wb'))\n",
    "\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=True)\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=False)\n",
    "\n",
    "\n",
    "    def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "        if train:\n",
    "            dataset_type = \"Train\"\n",
    "            data, labels = x_train, y_train\n",
    "        else:\n",
    "            dataset_type = \"Test\"\n",
    "            data, labels = x_test, y_test\n",
    "\n",
    "        y_prediction = clf.predict(data)\n",
    "        clf_report = classification_report(labels, y_prediction)\n",
    "        accuracy = accuracy_score(labels, y_prediction) * 100\n",
    "        confusion_mat = confusion_matrix(labels, y_prediction)\n",
    "\n",
    "        print(f\"{dataset_type} Result:\\n{'=' * 50}\")\n",
    "        print(f\"Accuracy Score: {accuracy:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n{confusion_mat}\\n\")\n",
    "\n",
    "\n",
    "    def k_fold(self):\n",
    "        print(f\"[INFO] Evaluating mode: {self.feature}\")\n",
    "\n",
    "        for n_splits in self.n_splits_values:\n",
    "            cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "            scores = cross_val_score(self.best_estimator, self.data, self.labels, scoring='accuracy', cv=cv, n_jobs=2)\n",
    "            print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718614327.749424     196 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1718614327.775423   21397 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1718614327.800927   21397 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "feature_extraction_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "nimhchefs = Dataset(data_path=CURRENT_PATH, img_size=64, transform=feature_extraction_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVMClassifier(dataset=nimhchefs, feature=FEATURE, img_size=64, **hog_params)\n",
    "\n",
    "svm_classifier.train()\n",
    "\n",
    "svm_classifier.k_fold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
