{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, shutil, random, cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe import Image\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# general paths\n",
    "BASE_PATH = Path('/project/volume/data/out')\n",
    "MODEL_PATH = Path('/project/volume/models')\n",
    "\n",
    "###################\n",
    "# SET DATASET HERE:\n",
    "DATA = FER\n",
    "###################\n",
    "\n",
    "# dataset specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "IMAGE_PATHS_STR = [str(path) for path in IMAGE_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    fig.add_subplot(1, 10, i + 1)\n",
    "    plt.imshow(np.array(cv2.imread(str(IMAGE_PATHS[i]))), cmap='gray')\n",
    "    label = Path(IMAGE_PATHS[i]).parent.name\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_per_cat = Counter(Path(file).parent.name for file in IMAGE_PATHS)\n",
    "categories = list(nbr_per_cat.keys())\n",
    "counts = list(nbr_per_cat.values())\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "bars = plt.bar(categories, counts, width=0.5)\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Number of Files per Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count), \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mp_cv2_image(image_path):\n",
    "    return Image.create_from_file(str(image_path)), cv2.imread(str(image_path))\n",
    "\n",
    "def preprocess_image(file, img_size):\n",
    "        img = cv2.imread(str(file))\n",
    "        imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return cv2.resize(imgray, (img_size, img_size))\n",
    "\n",
    "def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "    if train:\n",
    "        y_prediction = clf.predict(x_train)\n",
    "        clf_report = classification_report(y_train, y_prediction)\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, y_prediction)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        y_prediction = clf.predict(x_test)\n",
    "        clf_report = classification_report(y_test, y_prediction)\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_prediction)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_mesh = '/project/volume/models/face_landmarker.task'\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark extraction\n",
    "def extract_fetures(feature='pixels', img_size=64, **kwargs):\n",
    "\n",
    "    labels = []\n",
    "    data = []\n",
    "\n",
    "    for file in tqdm(IMAGE_PATHS):\n",
    "        \n",
    "        img_cv2 = preprocess_image(file, img_size)\n",
    "\n",
    "        if feature == 'landmark':\n",
    "                \n",
    "            print(\"[INFO] Extracting facial landmarks ...\")\n",
    "\n",
    "            rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "            detection_result = detector_mesh.detect(rgb_frame)\n",
    "\n",
    "            if detection_result.face_landmarks:\n",
    "                array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                labels.append(file.parent.name)\n",
    "                data.append(array)\n",
    "\n",
    "        elif feature == 'pixel':\n",
    "                \n",
    "            print(\"[INFO] Extracting image pixels ...\")\n",
    "                    \n",
    "            labels.append(file.parent.name)\n",
    "            data.append(np.array(img_cv2).flatten())\n",
    "\n",
    "        elif feature == 'hog':\n",
    "\n",
    "            print(\"[INFO] Extracting hog feature vectors ...\")\n",
    "\n",
    "            orientations = kwargs.get('orientations', None)\n",
    "            image_shape = kwargs.get('image_shape', None)\n",
    "            pixels_per_cell = kwargs.get('pixels_per_cell', None)\n",
    "            cells_per_block = kwargs.get('cells_per_block', None)\n",
    "\n",
    "            if orientations is None or image_shape is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                raise ValueError(\"orientations, image_shape, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "            def compute_hog_feature_size(image_shape, orientations, pixels_per_cell, cells_per_block):\n",
    "\n",
    "                height, width = image_shape, image_shape\n",
    "                num_cells_x = width // pixels_per_cell\n",
    "                num_cells_y = height // pixels_per_cell\n",
    "                num_blocks_x = num_cells_x - cells_per_block + 1\n",
    "                num_blocks_y = num_cells_y - cells_per_block + 1\n",
    "                features_per_block = cells_per_block * cells_per_block * orientations\n",
    "                total_features = num_blocks_x * num_blocks_y * features_per_block\n",
    "\n",
    "                print(\"[INFO] Size of HOG feature vector ...\", total_features)\n",
    "\n",
    "            compute_hog_feature_size(image_shape, orientations, pixels_per_cell, cells_per_block)\n",
    "\n",
    "                \n",
    "            fd1 = hog(\n",
    "                img_cv2, orientations=orientations, \n",
    "                pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                cells_per_block=(cells_per_block, cells_per_block),\n",
    "                block_norm='L2-Hys',\n",
    "                transform_sqrt=False, \n",
    "                feature_vector=True\n",
    "                )\n",
    "\n",
    "            label = Path(p).parent.name\n",
    "            labels.append(label)\n",
    "            data.append(fd1)\n",
    "\n",
    "        elif feature == 'blendshape':\n",
    "                \n",
    "            print(\"[INFO] Extracting facial blendshapes ...\")\n",
    "            \n",
    "            rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "            detection_result = detector_mesh.detect(rgb_frame)\n",
    "\n",
    "            if detection_result.face_blendshapes:\n",
    "                array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                labels.append(file.parent.name)\n",
    "                data.append(array)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feature type: {feature}! Should be one of: [blendshape, pixel, landmark, hog]\")\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = 'hog'\n",
    "\n",
    "hog_params = { \n",
    "    'orientations' : 7,\n",
    "    'pixels_per_cell' : 8,\n",
    "    'cells_per_block' : 4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = extract_fetures(feature=FEATURE, img_size=64, **hog_params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42)\n",
    "print(\"[INFO] Number of images used in training ...\", x_train.shape[0])\n",
    "print(\"[INFO] Number of images used in testing ...\", x_test.shape[0])\n",
    "\n",
    "classifier = SVC()\n",
    "parameters = {'gamma': [0.1, 0.01, 0.001], 'C': [1, 10, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "pickle.dump(best_estimator, open(str(MODEL_PATH / DATA / f'{FEATURE}_model.p'), 'wb'))\n",
    "\n",
    "print_score(best_estimator, x_train, y_train, x_test, y_test, train=True)\n",
    "print_score(best_estimator, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_values = [3, 5, 10]\n",
    "\n",
    "print(f\"[INFO] Evaluating mode: {FEATURE}\")\n",
    "\n",
    "for n_splits in n_splits_values:\n",
    "    cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    scores = cross_val_score(best_estimator, data, labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
