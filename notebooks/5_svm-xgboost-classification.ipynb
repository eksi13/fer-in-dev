{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    import cv2\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import sklearn.svm\n",
    "    import mediapipe as mp\n",
    "      \n",
    "except:\n",
    "    %pip install opencv-python-headless==4.9.0.80\n",
    "    %pip install torch\n",
    "    %pip install torchvision\n",
    "    %pip install torchsummary \n",
    "    %pip install sklearn\n",
    "    %pip install mediapipe\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import cuda\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "\n",
    "# scikit learn\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# mediapipe\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# standard libraries\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Protobuf imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install protobuf for mediapipe\n",
    "%pip uninstall protobuf -y\n",
    "#%pip install protobuf==4.25.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install protobuf==4.25.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Data Path & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "\n",
    "EMOREACT = 'EmoReact'\n",
    "FER = 'FER-2013'\n",
    "KDEF = 'KDEF-AKDEF'\n",
    "NIMH = 'NIMH-CHEFS'\n",
    "INTERNAL = Path('/home/jovyan/work/output/extracted_faces')\n",
    "\n",
    "DATASET = 'internal'\n",
    "\n",
    "DATA_PATH = Path('/home/jovyan/work/data/out/') / DATASET\n",
    "DATA_PATH = INTERNAL\n",
    "\n",
    "# Dataset-specific paths\n",
    "LABELS = [f.name for f in DATA_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(DATA_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Model parameters\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Constants for feature extraction\n",
    "FEATURES = 'feature-extraction'\n",
    "TRANSFER = 'transfer-learning'\n",
    "FINETUNE = 'fine-tuning'\n",
    "\n",
    "# Cuda parameters\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "# feature extaraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, phase, img_size=64, transform=None, model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):\n",
    "        self.data_path = Path(data_path) / phase\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform[phase]\n",
    "        self.phase = phase\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.class_to_int = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "        self.int_to_class = {idx: class_name for class_name, idx in self.class_to_int.items()}\n",
    "\n",
    "        # feature extraction\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # mediapipe\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=True, output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in self.data_path.iterdir() if f.is_dir()]\n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle(paths)\n",
    "        return paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = Path(img_path).parent.name\n",
    "        label = self.class_to_int[label]  # convert label to integer\n",
    "        return img, label\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        return img\n",
    "\n",
    "    def idx_to_class(self, idx_list):\n",
    "        return [self.int_to_class[idx] for idx in idx_list]\n",
    "\n",
    "    def class_to_idx(self, class_list):\n",
    "        return [self.class_to_int[class_name] for class_name in class_list]\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_cv2 = self.get_cv2_img(idx)\n",
    "        plt.imshow(img_cv2, cmap='gray')\n",
    "        plt.show()\n",
    "    \n",
    "    def extract_features(self, feature):\n",
    "        print(f\"[INFO] Extracting {feature} vectors ...\")\n",
    "        labels = []\n",
    "        data = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self))):\n",
    "            img, label = self[idx]\n",
    "            img_cv2 = self.get_cv2_img(idx)\n",
    "\n",
    "           # print(img_cv2.shape)\n",
    "\n",
    "            if feature == 'landmarks':\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif feature == 'pixel':\n",
    "                labels.append(label)\n",
    "                img_array = np.array(img).flatten()\n",
    "                data.append(img_array)\n",
    "\n",
    "            elif feature == 'hog':\n",
    "                orientations = self.hog_args['hog_params'].get('orientations', None)\n",
    "                pixels_per_cell = self.hog_args['hog_params'].get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args['hog_params'].get('cells_per_block', None)\n",
    "                \n",
    "                if orientations is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True,\n",
    "                    channel_axis=-1\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "                #if len(fd1) != 94192:\n",
    "                #    print(idx)\n",
    "                #    return\n",
    "                #print(len(fd1))\n",
    "\n",
    "            elif feature == 'blendshapes':\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {feature}! Should be one of: [blendshapes, pixel, landmark, hog]\")\n",
    "\n",
    "        data = np.array(data)\n",
    "        labels = np.array(labels)\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        #transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        #transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = { x: Dataset(DATA_PATH, phase=x, img_size=256, transform=data_transforms, hog_params=hog_params) for x in [TRAIN, VAL, TEST] }\n",
    "dataloaders = { x: torch.utils.data.DataLoader(datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in [TRAIN, VAL, TEST] }\n",
    "dataset_sizes = { x : len(datasets[x]) for x in [TRAIN, VAL, TEST] }\n",
    "n_classes = datasets[TRAIN].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[TRAIN][0][0].shape)\n",
    "\n",
    "img_cv2 = datasets[TRAIN].get_cv2_img(101)\n",
    "\n",
    "print(img_cv2.shape)\n",
    "\n",
    "fd1 = hog(\n",
    "    img_cv2, orientations=orientations, \n",
    "    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "    cells_per_block=(cells_per_block, cells_per_block),\n",
    "    block_norm='L2-Hys',\n",
    "    transform_sqrt=False, \n",
    "    feature_vector=True,\n",
    "    channel_axis=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Init VGG16 for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the VGG model\n",
    "vgg16 = models.vgg16(weights='IMAGENET1K_V1')\n",
    "vgg16 = vgg16.to(DEVICE)\n",
    "\n",
    "# freeze parameters for feature extraction\n",
    "for param in vgg16.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# remove classifer to output features\n",
    "vgg16.classifier = torch.nn.Identity()\n",
    "vgg16.classifier = vgg16.classifier.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the feature extraction method\n",
    "def extract_vgg_features(loader, conv_base):\n",
    "    conv_base.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, targets) in loader:\n",
    "            # move data to cuda\n",
    "            images = images.to(DEVICE)\n",
    "            targets = torch.as_tensor(targets).to(DEVICE)\n",
    "            \n",
    "            # Extract features using conv_base\n",
    "            features_batch = conv_base(images)\n",
    "            features.append(features_batch.cpu().numpy())\n",
    "            labels.append(targets.cpu().numpy())\n",
    "\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = 'hog'\n",
    "VGG = False\n",
    "\n",
    "if VGG:\n",
    "    FEATURE = 'VGG'\n",
    "    since = time.time()\n",
    "    # extract features\n",
    "    train_features, train_labels = extract_vgg_features(loader=dataloaders[TRAIN], conv_base=vgg16)\n",
    "    test_features, test_labels = extract_vgg_features(loader=dataloaders[TEST], conv_base=vgg16)\n",
    "    val_features, val_labels = extract_vgg_features(loader=dataloaders[VAL], conv_base=vgg16)\n",
    "    elapsed_time = time.time() - since\n",
    "    print(f\"Feature Extraction completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    # combine into one dataset\n",
    "    vgg_features = np.concatenate((train_features, test_features, val_features))\n",
    "    vgg_labels = np.concatenate((train_labels, test_labels, val_labels))\n",
    "    \n",
    "    print(vgg_features.shape, vgg_labels.shape)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(vgg_features, vgg_labels, test_size=0.2, shuffle=True, stratify=vgg_labels, random_state=42)\n",
    "\n",
    "else:\n",
    "    # extract features\n",
    "    since = time.time()\n",
    "    \n",
    "    manual_train_features, manual_train_labels = datasets[TRAIN].extract_features(feature=FEATURE)\n",
    "    manual_test_features, manual_test_labels = datasets[TEST].extract_features(feature=FEATURE)\n",
    "    manual_val_features, manual_val_labels = datasets[VAL].extract_features(feature=FEATURE)\n",
    "    manual_features = np.concatenate((manual_train_features, manual_test_features, manual_val_features))\n",
    "    manual_labels = np.concatenate((manual_train_labels, manual_test_labels, manual_val_labels))\n",
    "    \n",
    "    elapsed_time = time.time() - since\n",
    "    print(f\"Feature Extraction completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    \n",
    "    print(manual_features.shape, manual_labels.shape)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(manual_features, manual_labels, test_size=0.2, shuffle=True, stratify=manual_labels, random_state=42)\n",
    "\n",
    "# create train and test sets \n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, x_train, y_train, x_test, y_test, mode, train=True):\n",
    "    if train:\n",
    "        y_prediction = clf.predict(x_train)\n",
    "        clf_report = classification_report(y_train, y_prediction)\n",
    "        train_result = (\n",
    "            \"Train Result:\\n\"\n",
    "            \"================================================\\n\"\n",
    "            f\"Accuracy Score: {accuracy_score(y_train, y_prediction) * 100:.2f}%\\n\"\n",
    "            \"_______________________________________________\\n\"\n",
    "            f\"CLASSIFICATION REPORT:\\n{clf_report}\\n\"\n",
    "            \"_______________________________________________\\n\"\n",
    "            f\"Confusion Matrix: \\n {confusion_matrix(y_train, y_prediction)}\\n\\n\"\n",
    "        )\n",
    "        Path('/home/jovyan/work/reports').mkdir(parents=True, exist_ok=True)\n",
    "        (Path(f'/home/jovyan/work/reports/training_results_{mode}.txt')).write_text(train_result)\n",
    "\n",
    "        print(train_result)\n",
    "        \n",
    "    elif train==False:\n",
    "        y_prediction = clf.predict(x_test)\n",
    "        clf_report = classification_report(y_test, y_prediction)\n",
    "        test_result = (\n",
    "            \"Test Result:\\n\"\n",
    "            \"================================================\\n\"\n",
    "            f\"Accuracy Score: {accuracy_score(y_test, y_prediction) * 100:.2f}%\\n\"\n",
    "            \"_______________________________________________\\n\"\n",
    "            f\"CLASSIFICATION REPORT:\\n{clf_report}\\n\"\n",
    "            \"_______________________________________________\\n\"\n",
    "            f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_prediction)}\\n\\n\"\n",
    "        )\n",
    "        Path('/home/jovyan/work/reports').mkdir(parents=True, exist_ok=True)\n",
    "        (Path(f'/home/jovyan/work/reports/test_results_{mode}.txt')).write_text(test_result)\n",
    "\n",
    "        print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['poly'],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "\n",
    "halving_random_search = HalvingRandomSearchCV(SVC(), param_grid, n_candidates='exhaust', refit=True, verbose=3, cv=5, factor=2, random_state=42)\n",
    "halving_random_search.fit(x_train, y_train)\n",
    "print(\"Best Parameters:\", halving_random_search.best_params_)\n",
    "best_model = halving_random_search.best_estimator_\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "elapsed_time = time.time() - since\n",
    "print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for DATASET: {DATASET}, FEATURE: {FEATURE}\")\n",
    "print(\"Best Parameters:\", halving_random_search.best_params_)\n",
    "print_score(best_model, x_train, y_train, x_test, y_test, mode='SVM_NIMH-CHEFS', train=False)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(best_model, manual_features, manual_labels, scoring='accuracy', cv=kf, n_jobs=2)\n",
    "print(f\"10-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\")\n",
    "\n",
    "pkl_name = f'{DATASET}_SVM_{FEATURE}_{halving_random_search.best_params_}.pkl'\n",
    "pkl_path = str(MODEL_PATH / 'PICKLE' / pkl_name)\n",
    "with open(pkl_path, 'wb') as f:\n",
    "   pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "# Initialize the HalvingRandomSearchCV\n",
    "halving_random_search = HalvingRandomSearchCV(\n",
    "    XGBClassifier(eval_metric='logloss'), \n",
    "    param_grid, \n",
    "    n_candidates='exhaust', \n",
    "    refit=True, \n",
    "    verbose=3, \n",
    "    cv=5, \n",
    "    factor=2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "since = time.time()\n",
    "halving_random_search.fit(x_train, y_train)\n",
    "print(\"Best Parameters:\", halving_random_search.best_params_)\n",
    "best_model = halving_random_search.best_estimator_\n",
    "y_pred = best_model.predict(x_test)\n",
    "elapsed_time = time.time() - since\n",
    "print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for DATASET: {DATASET}, FEATURE: {FEATURE}\")\n",
    "print(\"Best Parameters:\", halving_random_search.best_params_)\n",
    "print_score(best_model, x_train, y_train, x_test, y_test, mode='XGBOOST_NIMH-CHEFS', train=False)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(best_model, x_test, y_test, scoring='accuracy', cv=kf, n_jobs=2)\n",
    "print(f\"10-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
