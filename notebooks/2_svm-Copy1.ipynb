{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import cv2\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import sklearn.svm\n",
    "    import mediapipe as mp\n",
    "except:\n",
    "    %pip install opencv-python-headless==4.9.0.80\n",
    "    %pip install torch\n",
    "    %pip install torchvision\n",
    "    %pip install torchsummary \n",
    "    %pip install sklearn\n",
    "    %pip install mediapipe\n",
    "\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Feature extraction method\n",
    "FEATURE = 'pixel'\n",
    "\n",
    "# Parameters for Histogram of Oriented Gradients (HOG) feature extraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}\n",
    "len(IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = BASE_PATH / DATA / VAL / Path('Curiosity')\n",
    "imgs = list(path.rglob('*.jpg'))\n",
    "print(len(imgs))\n",
    "\n",
    "random.shuffle(imgs)\n",
    "\n",
    "sampled_imgs = imgs[:38]\n",
    "files_to_delete = set(imgs) - set(sampled_imgs)\n",
    "\n",
    "for file in files_to_delete:\n",
    "    file.unlink()\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size=64, transform=None, model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # mediapipe\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=True, \n",
    "                                                         output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path / TRAIN).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            _, label = self.__getitem__(i)\n",
    "            img_cv2 = self.get_cv2_img(i)\n",
    "\n",
    "            ax.imshow(img_cv2, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_cv2 = self.get_cv2_img(idx)\n",
    "        plt.imshow(img_cv2, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self, save_plot=True):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "        \n",
    "        if save_plot:\n",
    "            figure_path = Path('/home/jovyan/work') / Path('distributions') \n",
    "            if not figure_path.exists():\n",
    "                figure_path.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(str(figure_path / self.data_path.name))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img_cv2 = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2GRAY)\n",
    "        img_cv2 = cv2.resize(img_cv2, dsize=(self.img_size, self.img_size))\n",
    "        return img_cv2\n",
    "\n",
    "    \n",
    "    def extract_features(self, feature):\n",
    "        print(f\"[INFO] Extracting {feature} vectors ...\")\n",
    "\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for idx in tqdm(range(len(self))):\n",
    "            \n",
    "            img, label = self[idx]\n",
    "            img_cv2 = self.get_cv2_img(idx)\n",
    "\n",
    "            if feature == 'landmarks':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif feature == 'pixels':\n",
    "                        \n",
    "                labels.append(label)\n",
    "                img_array = np.array(img).flatten()\n",
    "                data.append(img_array)\n",
    "\n",
    "            elif feature == 'hog':\n",
    "                \n",
    "                orientations = self.hog_args['hog_params'].get('orientations', None)\n",
    "                pixels_per_cell = self.hog_args['hog_params'].get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args['hog_params'].get('cells_per_block', None)\n",
    "\n",
    "                if orientations is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "\n",
    "            elif feature == 'blendshapes':\n",
    "                \n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {feature}! Should be one of: [blendshapes, pixel, landmark, hog]\")\n",
    "        \n",
    "        data = np.array(data)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "\n",
    "    def to_df(self, to_csv=False):\n",
    "        df = pd.DataFrame()\n",
    "        df[FEATURE] = [row for row in self.data.reshape(len(self.dataset), -1)]\n",
    "        df['emotion'] = [value.item() for value in self.labels.reshape(-1, 1)]\n",
    "        if to_csv:\n",
    "            df.to_csv('features.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720874162.728402   82578 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n",
      "W0000 00:00:1720874162.729567   82578 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1720874162.737352   85830 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1720874162.748872   85841 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "feature_extraction_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "dataset = Dataset(data_path=CURRENT_PATH, img_size=128, transform=feature_extraction_transform, hog_params=hog_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, data, labels, feature, n_splits_values = [10], **kwargs):   \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.feature = feature\n",
    "        self.n_split_values = n_splits_values\n",
    "        self.best_estimator = None\n",
    "\n",
    "    def train(self):\n",
    "        since = time.time()\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.2, shuffle=True, stratify=self.labels, random_state=42)\n",
    "        \n",
    "        print(\"init\")\n",
    "        classifier = SVC()\n",
    "        \n",
    "        # Fit the classifier on the training data\n",
    "        classifier.fit(x_train, y_train)\n",
    "        self.best_estimator = classifier\n",
    "\n",
    "        model_path = str(MODEL_PATH / DATA)\n",
    "\n",
    "        Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        pickle.dump(self.best_estimator, open(model_path + f'/{self.feature}_model.p', 'wb'))\n",
    "\n",
    "        elapsed_time = time.time() - since\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=True)\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=False)\n",
    "\n",
    "\n",
    "    def print_score(self, x_train, y_train, x_test, y_test, train=True):\n",
    "        if train:\n",
    "            dataset_type = \"Train\"\n",
    "            data, labels = x_train, y_train\n",
    "        else:\n",
    "            dataset_type = \"Test\"\n",
    "            data, labels = x_test, y_test\n",
    "\n",
    "        y_prediction = self.best_estimator.predict(data)\n",
    "        clf_report = classification_report(labels, y_prediction)\n",
    "        accuracy = accuracy_score(labels, y_prediction) * 100\n",
    "        confusion_mat = confusion_matrix(labels, y_prediction)\n",
    "\n",
    "        print(f\"{dataset_type} Result:\\n{'=' * 50}\")\n",
    "        print(f\"Accuracy Score: {accuracy:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n{confusion_mat}\\n\")\n",
    "\n",
    "\n",
    "    def k_fold(self):\n",
    "        print(f\"[INFO] Evaluating mode: {self.feature}\")\n",
    "\n",
    "        for n_splits in self.n_splits_values:\n",
    "            cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "            scores = cross_val_score(self.best_estimator, self.data, self.labels, scoring='accuracy', cv=cv, n_jobs=2)\n",
    "            print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting hog vectors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [00:16<00:00, 31.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#pixel_data, pixel_labels = dataset.extract_features(feature='pixels')\n",
    "\n",
    "#landmark_data, landmark_labels = dataset.extract_features(feature='landmarks')\n",
    "\n",
    "hog_data, hog_labels = dataset.extract_features(feature='hog')\n",
    "\n",
    "#blendshapes_data, blendshapes_labels = dataset.extract_features(feature='blendshapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 18928)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hog_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_svm = SVMClassifier(data=pixel_data, labels=pixel_labels, feature='pixel')\n",
    "pixel_svm.train()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_svm = SVMClassifier(data=landmark_data, labels=landmark_labels, feature='landmark')\n",
    "landmarks_svm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_svm = SVMClassifier(data=hog_data, labels=hog_labels, feature='hog')\n",
    "hog_svm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blendshapes_svm = SVMClassifier(data=blendshapes_data, labels=blendshapes_labels, feature='blendshapes')\n",
    "blendshapes_svm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.k_fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C=100)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
