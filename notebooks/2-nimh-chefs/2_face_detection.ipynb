{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fa5867-836e-477f-8f4c-76c0708ba6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import succesful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "    from mediapipe import solutions\n",
    "    from mediapipe.framework.formats import landmark_pb2\n",
    "except:\n",
    "    %pip install opencv-python-headless==4.9.0.80\n",
    "    %pip install mediapipe\n",
    "\n",
    "import cv2, sys, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Union\n",
    "import pandas as pd\n",
    "import re \n",
    "import concurrent.futures\n",
    "\n",
    "# mediapipe\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "print('import succesful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1bc6f-d3b8-4b52-b635-3031292218d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12645104-edfd-4448-bb0a-bc770e0e08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "\n",
    "def _normalized_to_pixel_coordinates(normalized_x: float, \n",
    "                                     normalized_y: float, \n",
    "                                     image_width: int,\n",
    "                                     image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "  def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "  if not (is_valid_normalized_value(normalized_x) and is_valid_normalized_value(normalized_y)):\n",
    "    return None\n",
    "\n",
    "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "  return x_px, y_px\n",
    "\n",
    "\n",
    "def visualize(image, detection_result) -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  annotated_image = image.copy()\n",
    "  height, width, _ = image.shape\n",
    "\n",
    "  for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in detection.keypoints:\n",
    "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
    "                                                     width, height)\n",
    "      color, thickness, radius = (0, 255, 0), 2, 2\n",
    "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    category_name = category.category_name\n",
    "    category_name = '' if category_name is None else category_name\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = category_name + ' (' + str(probability) + ')'\n",
    "    text_location = (MARGIN + bbox.origin_x, \n",
    "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65ada2d-4aef-40b3-862e-a82c7c17ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title('Face Blendshapes')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# extract landmarks to csv\n",
    "def extract_and_save_landmarks(c_df, img_cv2, idx):\n",
    "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "    detection_result = detector_mesh.detect(rgb_frame)\n",
    "    df = pd.DataFrame([(idx, p.name, index, point.x, point.y, point.z) for index, point in enumerate(detection_result.face_landmarks[0])], columns=['image_idx', 'image_name', 'landmark_idx', 'x', 'y', 'z'])\n",
    "    c_df = c_df.append(df)\n",
    "    return c_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5204188-ab13-466e-b1fe-795876cc1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(path, img):\n",
    "    cv2.imwrite(path, img)\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def read_cv2_image(image_path):\n",
    "    return cv2.imread(str(image_path))\n",
    "\n",
    "def read_mp_cv2_image(image_path):\n",
    "    return mp.Image.create_from_file(str(image_path)), cv2.imread(str(image_path))\n",
    "\n",
    "# convert png to jpg\n",
    "def png_to_jpg(image_path):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    jpg_path = str(image_path.with_suffix('.jpg'))\n",
    "    cv2.imwrite(jpg_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "    image_path.unlink()\n",
    "\n",
    "# rename JPG\n",
    "def rename_jpg_files(image_path):\n",
    "    image_path = Path(image_path)\n",
    "    new_path = image_path.with_suffix('.jpg')\n",
    "    image_path.rename(new_path)\n",
    "\n",
    "# mediapipe face detection & cropping \n",
    "def crop_faces(img_cv2, detection_results, out_path):\n",
    "    for d in detection_results.detections:  \n",
    "        bbox = d.bounding_box\n",
    "        origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "        cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "        #show_image(cropped_img)      \n",
    "        cv2.imwrite(str(out_path), cropped_img)\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f4679f-60d7-4e0a-a06b-b358671c278d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721129939.828423   47318 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1721129939.878438   47735 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1721129939.880398   47318 task_runner.cc:85] GPU suport is not available: INTERNAL: ; RET_CHECK failure (mediapipe/gpu/gl_context_egl.cc:77) display != EGL_NO_DISPLAYeglGetDisplay() returned error 0x300c\n",
      "W0000 00:00:1721129939.882571   47318 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1721129939.894462   47749 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1721129939.908978   47762 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "model_path_detect = '/home/jovyan/work/models/blaze_face_short_range.tflite'\n",
    "model_path_mesh = '/home/jovyan/work/models/face_landmarker.task'\n",
    "\n",
    "base_options_detect = python.BaseOptions(model_asset_path=model_path_detect)\n",
    "options_detect = vision.FaceDetectorOptions(base_options=base_options_detect)\n",
    "detector_detect = vision.FaceDetector.create_from_options(options_detect)\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=2, min_face_detection_confidence=0.1)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0407795-8dc7-423d-96be-20b7528c8757",
   "metadata": {},
   "source": [
    "### Actual things happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43845e05-6559-464a-bac8-7a3d0a09d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4893 images\n"
     ]
    }
   ],
   "source": [
    "PATH = Path('/home/jovyan/work/data/out/KDEF-AKDEF')\n",
    "IMAGES = list(PATH.rglob('*.jpg'))\n",
    "\n",
    "OUT_PATH = Path('/home/jovyan/work/data/out/KDEF-AKDEF-faces')\n",
    "OUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'found {len(IMAGES)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cab8ce9-17c6-4a0e-ba16-c611eb42fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data/out/KDEF-AKDEF/test/Afraid/AF01AFFR.jpg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6db78d6-850e-4bb7-b882-0a1272def549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4893/4893 [05:34<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for image in tqdm(IMAGES):\n",
    "    \n",
    "    img_mp, img_cv2 = read_mp_cv2_image(str(image))\n",
    "    detection_results = detector_detect.detect(img_mp) \n",
    "    \n",
    "    for detection in detection_results.detections:   \n",
    "        bbox = detection.bounding_box\n",
    "        origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "        cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "        cropped_img = cv2.resize(cropped_img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "        cv2.imwrite(image, cropped_img)\n",
    "    \n",
    "        #plt.imshow(cropped_img, cmap='gray')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef8e57-3764-42f1-8a67-04d40add7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in tqdm(IMAGES):\n",
    "    # set labels\n",
    "    name = image.name\n",
    "    #print(name)\n",
    "    mu_label = name[:-11] + '.jpg'\n",
    "    ki_label =  name[:-18] + name[-11:-4] +'.jpg'\n",
    "    labels =[Path(mu_label), Path(ki_label)]\n",
    "    \n",
    "    # read image and detect faces\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(str(image))\n",
    "    detection_results = detector_detect.detect(img_mp)\n",
    "        \n",
    "    # get all detected faces and sort according to x-coordinate (mother is always left)\n",
    "    detections = [detection for detection in detection_results.detections]\n",
    "    sorted_faces = sorted(detections, key=lambda d: d.bounding_box.origin_x)\n",
    "\n",
    "    for(index, detection) in enumerate(sorted_faces):\n",
    "        try:\n",
    "            bbox = detection.bounding_box\n",
    "            origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "            cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "            label = labels[index]\n",
    "            if re.search('0.0', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(NEUTRAL) / label)\n",
    "            elif re.search('1.1', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(NEGATIVE) / label)\n",
    "            elif re.search('1.2', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(POSITIVE) / label)\n",
    "            else:\n",
    "                pass\n",
    "            #print(res_path)\n",
    "            cv2.imwrite(res_path, cropped_img)\n",
    "            #show_image(cropped_img)  \n",
    "        except:\n",
    "            print(f'Error processing {name}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e128ade-b2a2-4fbc-88ae-9d6810fc3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/work/output/extracted_faces/START_S001_T1_La1_frame_0_timestamp_27.92_KI_0.0.jpg'\n",
    "\n",
    "img_mp, img_cv2 = read_mp_cv2_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c470646-f3e2-4c30-a392-ae8bed80a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face mesh\n",
    "rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "mesh_results = detector_mesh.detect(rgb_frame)\n",
    "annotated_image = draw_landmarks_on_image(rgb_frame.numpy_view(), mesh_results)\n",
    "res_img = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "show_image(res_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
