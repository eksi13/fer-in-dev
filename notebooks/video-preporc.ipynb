{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "#from prettytable import PrettyTable\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import pickle, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('/project/volume/data/in/EmoReact')\n",
    "LABELS_PATH = Path('/project/volume/data/in/EmoReact/EmoReact_V_1.0/Labels/')\n",
    "ORIG_FOLDERS = ['Test', 'Train', 'Validation']\n",
    "LABEL_FILES = ['test_labels.text', 'train_labels.text/', 'val_labels.text']\n",
    "\n",
    "labels = ['Curiosity', 'Uncertainty', 'Excitement', 'Happiness', 'Surprise', 'Disgust', 'Fear', 'Frustration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders():\n",
    "    for label in labels:\n",
    "        label_path = BASE_PATH / Path(label)\n",
    "        if not label_path.exists():\n",
    "            label_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_labels():\n",
    "    df_train, df_test, df_val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    for label in LABEL_FILES:\n",
    "        path = LABELS_PATH / Path(label)\n",
    "        df = pd.read_csv(str(path), header=None)\n",
    "        df = df.drop(columns=[df.columns[-1]])\n",
    "        df.columns = labels\n",
    "        if 'test_labels.text' in label:\n",
    "            df_train = pd.concat([df_train, df], ignore_index=True)\n",
    "        elif 'train_labels.text' in label:\n",
    "            df_test = pd.concat([df_test, df], ignore_index=True)\n",
    "        elif 'val_labels.text' in label:\n",
    "            df_val = pd.concat([df_val, df], ignore_index=True)\n",
    "\n",
    "    return df_train, df_test, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_val = read_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets():\n",
    "    for folder in tqdm(ORIG_FOLDERS):\n",
    "        folder_path = BASE_PATH / Path('EmoReact_V_1.0/Data') / folder\n",
    "        if 'Test' in folder:\n",
    "            df = df_test\n",
    "        elif 'Train' in folder:\n",
    "            df = df_train\n",
    "        elif 'Validation' in folder:\n",
    "            df = df_val\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for file_idx, file in enumerate(list(folder_path.rglob('*.mp4'))):\n",
    "            if file_idx >= len(df):\n",
    "                break\n",
    "            file_label = df.loc[file_idx].idxmax() \n",
    "            destination_folder = BASE_PATH / Path(file_label)\n",
    "            destination_folder.mkdir(exist_ok=True, parents=True)\n",
    "            shutil.move(str(file), str(destination_folder))\n",
    "        \n",
    "        if folder_path.exists() and folder_path.is_dir():\n",
    "            shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_lenght():\n",
    "    video_lengths = []\n",
    "    for file in tqdm(list(BASE_PATH.rglob('*.mp4'))):\n",
    "        cap = cv2.VideoCapture(str(file))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count / fps\n",
    "        video_lengths.append(duration)\n",
    "        cap.release()\n",
    "    return video_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_lengths = get_video_lenght()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length = np.mean(video_lengths)\n",
    "std_dev = np.std(video_lengths)\n",
    "\n",
    "print(average_length, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames():\n",
    "    for p in tqdm(list(BASE_PATH.rglob('*.mp4'))):\n",
    "        cap = cv2.VideoCapture(str(p))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        nbr_frames = int(total_frames * 0.25)\n",
    "\n",
    "        for frame_idx in range(nbr_frames):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            _, frame = cap.read()\n",
    "            frame_file_name = Path(str(p.stem) + f'_{frame_idx}.jpg')\n",
    "            frame_path = str(p.parent / frame_file_name)\n",
    "            if not os.path.exists(str(frame_path)):\n",
    "                cv2.imwrite(frame_path, frame)\n",
    "    \n",
    "        cap.release()\n",
    "        if os.path.exists(str(p)):\n",
    "            os.remove(str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path('/project/volume/data/out/EmoReact')\n",
    "\n",
    "IMAGE_PATHS = list(BASE_PATH.rglob('*.jpg'))\n",
    "IMAGE_PATHS_STR = [str(path) for path in IMAGE_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import Image\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_detect = '/project/volume/models/blaze_face_short_range.tflite'\n",
    "model_path_mesh = '/project/volume/models/face_landmarker.task'\n",
    "\n",
    "base_options_detect = python.BaseOptions(model_asset_path=model_path_detect)\n",
    "options_detect = vision.FaceDetectorOptions(base_options=base_options_detect)\n",
    "detector_detect = vision.FaceDetector.create_from_options(options_detect)\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop = 0.6\n",
    "test_prop = 0.2\n",
    "valid_prop = 0.2\n",
    "\n",
    "number_of_images = len(list(BASE_PATH.rglob('*.jpg')))\n",
    "\n",
    "print(\"[INFO] Number of images in total ...\" + str(number_of_images))\n",
    "\n",
    "n_train = int((number_of_images * train_prop) + 0.5)\n",
    "n_valid = int((number_of_images * valid_prop) + 0.5)\n",
    "n_test = number_of_images - n_train - n_valid\n",
    "\n",
    "\n",
    "print(f\"[INFO] Number of images used in training ... {str(n_train)} ({str(train_prop * 100)}%)\")\n",
    "print(f\"[INFO] Number of images used in testing ... {str(n_test)} ({str(test_prop * 100)}%)\")\n",
    "print(f\"[INFO] Number of images used in validation ...{str(n_valid)} ({str(valid_prop * 100)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_move():\n",
    "    for label in tqdm(labels):\n",
    "        folder_path = BASE_PATH / label\n",
    "        train_destination = BASE_PATH / \"train\" / label\n",
    "        val_destination = BASE_PATH / \"val\" / label\n",
    "        test_destination = BASE_PATH / \"test\" / label\n",
    "\n",
    "        train_destination.mkdir(parents=True, exist_ok=True)\n",
    "        val_destination.mkdir(parents=True, exist_ok=True)\n",
    "        test_destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        files = list(folder_path.rglob('*.jpg'))\n",
    "        random.shuffle(files)\n",
    "        train_n = (int((len(files) * train_prop) + 0.5))\n",
    "        val_n = (int((len(files) * valid_prop) + 0.2))\n",
    "        \n",
    "        for file_idx, file in enumerate(list(folder_path.rglob('*.jpg'))):\n",
    "            if file_idx < train_n:\n",
    "                shutil.move(str(file), train_destination)\n",
    "            elif file_idx < train_n + val_n:\n",
    "                shutil.move(str(file), val_destination)\n",
    "            else:\n",
    "                shutil.move(str(file), test_destination)\n",
    "\n",
    "        if folder_path.exists() and folder_path.is_dir():\n",
    "            shutil.rmtree(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_move()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_distribution():\n",
    "    nbr_per_cat = Counter(Path(file).parent.name for file in IMAGE_PATHS)\n",
    "    print(dict(nbr_per_cat))\n",
    "\n",
    "    categories = list(nbr_per_cat.keys())\n",
    "    counts = list(nbr_per_cat.values())\n",
    "\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    bars = plt.bar(categories, counts, width=0.5)\n",
    "\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Number of Files per Category')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count), \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "get_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def resize_image(img, size):\n",
    "    return cv2.resize(img, (size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "    if train:\n",
    "        y_prediction = clf.predict(x_train)\n",
    "        clf_report = classification_report(y_train, y_prediction)\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, y_prediction)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        y_prediction = clf.predict(x_test)\n",
    "        clf_report = classification_report(y_test, y_prediction)\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_prediction)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features():\n",
    "    hog_data = []\n",
    "    hog_labels = []\n",
    "\n",
    "    for p in tqdm(IMAGE_PATHS):\n",
    "        im = to_grayscale(str(p))\n",
    "        im = resize_image(im, 128)\n",
    "        \n",
    "        # extract hog feature descriptor\n",
    "        fd1 = hog(im, orientations=7, pixels_per_cell=(8, 8),\n",
    "                  cells_per_block=(4, 4), \n",
    "                  block_norm='L2-Hys',\n",
    "                  transform_sqrt=False,\n",
    "                  feature_vector=True)\n",
    "\n",
    "        label = Path(p).parent.name\n",
    "        hog_labels.append(label)\n",
    "        hog_data.append(fd1)\n",
    "\n",
    "    hog_data = np.array(hog_data)\n",
    "    hog_labels = np.array(hog_labels)\n",
    "    print(\"[INFO] Number of features ...\", str(hog_data.shape[1]))\n",
    "    print(\"[INFO] Number of labels ...\", str(hog_labels.shape[0]))\n",
    "    return hog_data, hog_labels\n",
    "\n",
    "def compute_hog_feature_size(image_shape, orientations, pixels_per_cell, cells_per_block):\n",
    "    height, width = image_shape, image_shape\n",
    "    num_cells_x = width // pixels_per_cell\n",
    "    num_cells_y = height // pixels_per_cell\n",
    "    num_blocks_x = num_cells_x - cells_per_block + 1\n",
    "    num_blocks_y = num_cells_y - cells_per_block + 1\n",
    "    features_per_block = cells_per_block * cells_per_block * orientations\n",
    "    total_features = num_blocks_x * num_blocks_y * features_per_block\n",
    "    print(\"[INFO] Size of HOG feature vector ...\", total_features)\n",
    "\n",
    "# Parameters\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "image_shape = 64\n",
    "\n",
    "compute_hog_feature_size(image_shape, orientations, pixels_per_cell, cells_per_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    fig.add_subplot(1, 10, i + 1)\n",
    "    plt.imshow(np.array(cv2.imread(str(IMAGE_PATHS[i]))), cmap='gray')\n",
    "    label = Path(IMAGE_PATHS[i]).parent.name\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_data, hog_labels = extract_hog_features()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(hog_data, hog_labels, test_size=0.2, shuffle=True, stratify=hog_labels, random_state=42)\n",
    "print(\"[INFO] Number of images used in training ...\", x_train.shape[0])\n",
    "print(\"[INFO] Number of images used in testing ...\", x_test.shape[0])\n",
    "\n",
    "classifier = SVC()\n",
    "parameters = {'gamma': [0.1, 0.01, 0.001], 'C': [1, 10, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_estimator_hog = grid_search.best_estimator_\n",
    "print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "pickle.dump(best_estimator_hog, open('/project/volume/models/EmoReact/hog_model.p', 'wb'))\n",
    "\n",
    "print_score(best_estimator_hog, x_train, y_train, x_test, y_test, train=True)\n",
    "print_score(best_estimator_hog, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mp_cv2_image(image_path):\n",
    "    return mp.Image.create_from_file(str(image_path)), cv2.imread(str(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_faces():\n",
    "    for file in IMAGE_PATHS:\n",
    "        img_mp, img_cv2 = read_mp_cv2_image(file)\n",
    "        detection_results = detector_detect.detect(img_mp)\n",
    "        file_stem = str(file.stem)\n",
    "        file_parent = file.parent\n",
    "\n",
    "        for d_idx, d in enumerate(detection_results.detections):\n",
    "            bbox = d.bounding_box\n",
    "            origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "            face = img_cv2[origin_y:origin_y + height, origin_x:origin_x + width]\n",
    "\n",
    "            frame_file_name = f\"{file_stem}_face_{d_idx}.jpg\"\n",
    "            frame_path = file_parent / frame_file_name\n",
    "\n",
    "            if not frame_path.exists():\n",
    "                cv2.imwrite(str(frame_path), face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "train_gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "valid_gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "test_gen = keras.preprocessing.image.ImageDataGenerator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
