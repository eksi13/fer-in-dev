{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79465d6f-94f4-4aee-8dcd-18427d7760fc",
   "metadata": {},
   "source": [
    "This notebooks detects and extracts the faces given by a provided dataset.\n",
    "It uses Mediapipe and CV2 to detect and crop the faces and seaves them to a specified output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6c5b3-41ad-468e-a67b-f5e6108dfbdb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa5867-836e-477f-8f4c-76c0708ba6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    from mediapipe.tasks import python\n",
    "    from mediapipe.tasks.python import vision\n",
    "    from mediapipe import solutions\n",
    "    from mediapipe.framework.formats import landmark_pb2\n",
    "except:\n",
    "    %pip install opencv-python-headless==4.9.0.80\n",
    "    %pip install mediapipe\n",
    "\n",
    "import cv2, sys, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Union\n",
    "import pandas as pd\n",
    "import re \n",
    "import concurrent.futures\n",
    "\n",
    "# mediapipe\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "print('import succesful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1bc6f-d3b8-4b52-b635-3031292218d8",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12645104-edfd-4448-bb0a-bc770e0e08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "\n",
    "def _normalized_to_pixel_coordinates(normalized_x: float, \n",
    "                                     normalized_y: float, \n",
    "                                     image_width: int,\n",
    "                                     image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "  def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "  if not (is_valid_normalized_value(normalized_x) and is_valid_normalized_value(normalized_y)):\n",
    "    return None\n",
    "\n",
    "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "  return x_px, y_px\n",
    "\n",
    "\n",
    "def visualize(image, detection_result) -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  annotated_image = image.copy()\n",
    "  height, width, _ = image.shape\n",
    "\n",
    "  for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in detection.keypoints:\n",
    "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
    "                                                     width, height)\n",
    "      color, thickness, radius = (0, 255, 0), 2, 2\n",
    "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    category_name = category.category_name\n",
    "    category_name = '' if category_name is None else category_name\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = category_name + ' (' + str(probability) + ')'\n",
    "    text_location = (MARGIN + bbox.origin_x, \n",
    "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title('Face Blendshapes')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# extract landmarks to csv\n",
    "def extract_and_save_landmarks(c_df, img_cv2, idx):\n",
    "    rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "    detection_result = detector_mesh.detect(rgb_frame)\n",
    "    df = pd.DataFrame([(idx, p.name, index, point.x, point.y, point.z) for index, point in enumerate(detection_result.face_landmarks[0])], columns=['image_idx', 'image_name', 'landmark_idx', 'x', 'y', 'z'])\n",
    "    c_df = c_df.append(df)\n",
    "    return c_df\n",
    "\n",
    "def save_image(path, img):\n",
    "    cv2.imwrite(path, img)\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def read_cv2_image(image_path):\n",
    "    return cv2.imread(str(image_path))\n",
    "\n",
    "def read_mp_cv2_image(image_path):\n",
    "    return mp.Image.create_from_file(str(image_path)), cv2.imread(str(image_path))\n",
    "\n",
    "# convert png to jpg\n",
    "def png_to_jpg(image_path):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    jpg_path = str(image_path.with_suffix('.jpg'))\n",
    "    cv2.imwrite(jpg_path, img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n",
    "    image_path.unlink()\n",
    "\n",
    "# rename JPG\n",
    "def rename_jpg_files(image_path):\n",
    "    image_path = Path(image_path)\n",
    "    new_path = image_path.with_suffix('.jpg')\n",
    "    image_path.rename(new_path)\n",
    "\n",
    "# mediapipe face detection & cropping \n",
    "def crop_faces(img_cv2, detection_results, out_path):\n",
    "    for d in detection_results.detections:  \n",
    "        bbox = d.bounding_box\n",
    "        origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "        cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "        #show_image(cropped_img)      \n",
    "        cv2.imwrite(str(out_path), cropped_img)\n",
    "        return cropped_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98d9ca-697d-4da1-be3a-57664b14b6b4",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4679f-60d7-4e0a-a06b-b358671c278d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path_detect = '/home/jovyan/work/models/blaze_face_short_range.tflite'\n",
    "model_path_mesh = '/home/jovyan/work/models/face_landmarker.task'\n",
    "\n",
    "base_options_detect = python.BaseOptions(model_asset_path=model_path_detect)\n",
    "options_detect = vision.FaceDetectorOptions(base_options=base_options_detect)\n",
    "detector_detect = vision.FaceDetector.create_from_options(options_detect)\n",
    "\n",
    "base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "options_mesh = vision.FaceLandmarkerOptions(base_options=base_options_mesh,\n",
    "                                       output_face_blendshapes=False,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=2, min_face_detection_confidence=0.7)\n",
    "detector_mesh = vision.FaceLandmarker.create_from_options(options_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946f1da-c489-4324-ba15-0ece0c7201c4",
   "metadata": {},
   "source": [
    "# Dataset parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7660b8d-4a19-4acc-a577-61a9eb77bd9d",
   "metadata": {},
   "source": [
    "Set path to dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035aac32-f5d1-4cc0-88e6-4ff065116730",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('/home/jovyan/work/cross-label/frames')\n",
    "IMAGES = list(PATH.rglob('*.jpg'))\n",
    "\n",
    "OUT_PATH = Path('/home/jovyan/work/cross-label/extracted_faces')\n",
    "OUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NEGATIVE = 'negative'\n",
    "POSITIVE = 'positive'\n",
    "NEUTRAL = 'neutral'\n",
    "\n",
    "print(f'found {len(IMAGES)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2827fd-5d47-4136-906c-70d604f1f102",
   "metadata": {},
   "source": [
    "Extract and save faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef8e57-3764-42f1-8a67-04d40add7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in tqdm(IMAGES):\n",
    "    # set labels\n",
    "    name = image.name\n",
    "    mu_label = name[:-11] + '.jpg'\n",
    "    ki_label =  name[:-18] + name[-11:-4] +'.jpg'\n",
    "    labels =[Path(mu_label), Path(ki_label)]\n",
    "    \n",
    "    # read image and detect faces\n",
    "    img_mp, img_cv2 = read_mp_cv2_image(str(image))\n",
    "    detection_results = detector_detect.detect(img_mp)\n",
    "        \n",
    "    # get all detected faces and sort according to x-coordinate (mother is always left)\n",
    "    detections = [detection for detection in detection_results.detections]\n",
    "    sorted_faces = sorted(detections, key=lambda d: d.bounding_box.origin_x)\n",
    "    \n",
    "    for(index, detection) in enumerate(sorted_faces):\n",
    "        try:\n",
    "            bbox = detection.bounding_box\n",
    "            origin_x, origin_y, width, height = bbox.origin_x, bbox.origin_y, bbox.width, bbox.height\n",
    "            cropped_img = img_cv2[origin_y:origin_y+height, origin_x:origin_x+width]\n",
    "            label = labels[index]\n",
    "            if re.search('0.0', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(NEUTRAL) / label)\n",
    "            elif re.search('1.1', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(NEGATIVE) / label)\n",
    "            elif re.search('1.2', str(label)):\n",
    "                res_path = str(OUT_PATH / Path(POSITIVE) / label)\n",
    "            else:\n",
    "                pass\n",
    "            cv2.imwrite(res_path, cropped_img)\n",
    "            #show_image(cropped_img)  \n",
    "        except:\n",
    "            print(f'Error processing {name}')\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
