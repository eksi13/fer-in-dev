{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "# Feature extraction method\n",
    "FEATURE = 'pixels'\n",
    "\n",
    "# Parameters for Histogram of Oriented Gradients (HOG) feature extraction\n",
    "orientations = 7\n",
    "pixels_per_cell = 8\n",
    "cells_per_block = 4\n",
    "\n",
    "hog_params = { \n",
    "    'orientations': orientations,\n",
    "    'pixels_per_cell': pixels_per_cell,\n",
    "    'cells_per_block': cells_per_block\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 5, 10, 20],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size=64, feature='pixels', transform=None, model_path_mesh='/home/jovyan/work/models/face_landmarker.task', **kwargs):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "\n",
    "        self.hog_args = kwargs\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # mediapipe\n",
    "        self.model_path_mesh = model_path_mesh\n",
    "        self.base_options_mesh = python.BaseOptions(model_asset_path=model_path_mesh)\n",
    "        self.options_mesh = vision.FaceLandmarkerOptions(base_options=self.base_options_mesh, output_face_blendshapes=False, \n",
    "                                                         output_facial_transformation_matrixes=True, num_faces=1)\n",
    "        self.detector_mesh = vision.FaceLandmarker.create_from_options(self.options_mesh)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path / TRAIN).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            img, label = self.__getitem__(i)\n",
    "            if img.ndim == 3:\n",
    "                img = img.squeeze(0)\n",
    "            img = img.numpy() if isinstance(img, torch.Tensor) else np.array(img)\n",
    "\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        img_cv2 = self.get_cv2_img(idx)\n",
    "        plt.imshow(img_cv2, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        return cv2.imread(str(img_path))\n",
    "    \n",
    "\n",
    "    def extract_features(self):\n",
    "        print(f\"[INFO] Extracting {self.feature} vectors ...\")\n",
    "\n",
    "        labels = []\n",
    "        data = []\n",
    "\n",
    "        for idx in tqdm(range(len(self.dataset))):\n",
    "            \n",
    "            img, label = self.dataset[idx]\n",
    "            img_cv2 = self.dataset.get_cv2_img(idx)\n",
    "\n",
    "            if self.feature == 'landmarks':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_landmarks:\n",
    "                    array = np.array([[lm.x, lm.y, lm.z] for lm in detection_result.face_landmarks[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "\n",
    "            elif self.feature == 'pixels':\n",
    "                        \n",
    "                labels.append(label)\n",
    "                img_array = np.array(img)\n",
    "                data.append(img_array)\n",
    "\n",
    "\n",
    "            elif self.feature == 'hog':\n",
    "                \n",
    "                orientations = self.hog_args.get('orientations', None)\n",
    "                image_shape = self.hog_args.get('image_shape', None)\n",
    "                pixels_per_cell = self.hog_args.get('pixels_per_cell', None)\n",
    "                cells_per_block = self.hog_args.get('cells_per_block', None)\n",
    "\n",
    "                if orientations is None or image_shape is None or pixels_per_cell is None or cells_per_block is None:\n",
    "                    raise ValueError(\"orientations, image_shape, pixels_per_cell and cells_per_block are required for hog feature extraction\")\n",
    "\n",
    "                fd1 = hog(\n",
    "                    img_cv2, orientations=orientations, \n",
    "                    pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "                    cells_per_block=(cells_per_block, cells_per_block),\n",
    "                    block_norm='L2-Hys',\n",
    "                    transform_sqrt=False, \n",
    "                    feature_vector=True\n",
    "                    )\n",
    "\n",
    "                labels.append(label)\n",
    "                data.append(fd1)\n",
    "\n",
    "            elif self.feature == 'blendshapes':\n",
    "\n",
    "                rgb_frame = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_cv2)\n",
    "                detection_result = self.detector_mesh.detect(rgb_frame)\n",
    "\n",
    "                if detection_result.face_blendshapes:\n",
    "                    array = np.array([[bs.index, bs.score] for bs in detection_result.face_blendshapes[0]]).flatten()\n",
    "                    labels.append(label)\n",
    "                    data.append(array)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"[Value Error] Unsupported feature type: {self.feature}! Should be one of: [blendshape, pixel, landmark, hog]\")\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "        return self.data, self.labels\n",
    "\n",
    "\n",
    "    def to_df(self, to_csv=False):\n",
    "        df = pd.DataFrame()\n",
    "        df[FEATURE] = [row for row in self.data.reshape(len(self.dataset), -1)]\n",
    "        df['emotion'] = [value.item() for value in self.labels.reshape(-1, 1)]\n",
    "        if to_csv:\n",
    "            df.to_csv('features.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTClassifier:\n",
    "    def __init__(self, dataset, feature='pixels', n_splits_values = [3, 5, 10], **kwargs):   \n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.n_split_values = n_splits_values\n",
    "        self.best_estimator = None\n",
    "        self.dt_params = kwargs\n",
    "\n",
    "    def train(self):\n",
    "        self.data, self.labels = self.extract_features()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(self.data, self.labels, test_size=0.2, shuffle=True, stratify=self.labels, random_state=42)\n",
    "\n",
    "        parameters = {\n",
    "            'criterion': self.dt_params.get('criterion', None),\n",
    "            'splitter': self.dt_params.get('splitter', None),\n",
    "            'max_depth': self.dt_params.get('max_depth', None),\n",
    "            'min_samples_split': self.dt_params.get('min_samples_split', None), \n",
    "            'min_samples_leaf': self.dt_params.get('critmin_samples_leaferion', None),\n",
    "            'max_features': self.dt_params.get('max_features', None),\n",
    "            'max_leaf_nodes': self.dt_params.get('max_leaf_nodes', None),\n",
    "            'min_impurity_decrease': self.dt_params.get('min_impurity_decrease', None),\n",
    "            'ccp_alpha': self.dt_params.get('ccp_alpha', None)\n",
    "        }\n",
    "\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        grid_search = GridSearchCV(classifier, parameters)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        self.best_estimator = grid_search.best_estimator_\n",
    "        print(\"[INFO] Best params ...\", grid_search.best_params_)\n",
    "\n",
    "        pickle.dump(self.best_estimator, open(str(MODEL_PATH / DATA / f'{self.feature}_model.p'), 'wb'))\n",
    "\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=True)\n",
    "        self.print_score(x_train, y_train, x_test, y_test, train=False)\n",
    "\n",
    "\n",
    "    def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "        if train:\n",
    "            dataset_type = \"Train\"\n",
    "            data, labels = x_train, y_train\n",
    "        else:\n",
    "            dataset_type = \"Test\"\n",
    "            data, labels = x_test, y_test\n",
    "\n",
    "        y_prediction = clf.predict(data)\n",
    "        clf_report = classification_report(labels, y_prediction)\n",
    "        accuracy = accuracy_score(labels, y_prediction) * 100\n",
    "        confusion_mat = confusion_matrix(labels, y_prediction)\n",
    "\n",
    "        print(f\"{dataset_type} Result:\\n{'=' * 50}\")\n",
    "        print(f\"Accuracy Score: {accuracy:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n{confusion_mat}\\n\")\n",
    "\n",
    "\n",
    "    def k_fold(self):\n",
    "        print(f\"[INFO] Evaluating mode: {self.feature}\")\n",
    "\n",
    "        for n_splits in self.n_splits_values:\n",
    "            cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "            scores = cross_val_score(self.best_estimator, self.data, self.labels, scoring='accuracy', cv=cv, n_jobs=2)\n",
    "            print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "nimhchefs = Dataset(data_path=CURRENT_PATH, img_size=64, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DTClassifier(dataset=nimhchefs, feature=FEATURE, img_size=64, **hog_params)\n",
    "\n",
    "dt_classifier.train()\n",
    "\n",
    "dt_classifier.k_fold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
