{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import cv2\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import sklearn.svm\n",
    "except:\n",
    "    %pip install opencv-python-headless==4.9.0.80\n",
    "    %pip install torch\n",
    "    %pip install torchvision\n",
    "    %pip install torchsummary \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import cuda\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "print('import successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train on gpu ...False\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "EMOREACT = Path('EmoReact')\n",
    "FER = Path('FER-2013')\n",
    "KDEF = Path('KDEF-AKDEF')\n",
    "NIMH = Path('NIMH-CHEFS')\n",
    "\n",
    "# General paths\n",
    "BASE_PATH = Path('/home/jovyan/work/data/out')\n",
    "MODEL_PATH = Path('/home/jovyan/work/models')\n",
    "\n",
    "# Set dataset here\n",
    "DATA = NIMH\n",
    "\n",
    "# Dataset-specific paths\n",
    "CURRENT_PATH = BASE_PATH / DATA\n",
    "LABELS = [f.name for f in CURRENT_PATH.iterdir() if f.is_dir()]\n",
    "IMAGE_PATHS = list(CURRENT_PATH.rglob('*.jpg'))\n",
    "\n",
    "# Constants for splitting dataset\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "\n",
    "FEATURES = 'feature-extraction'\n",
    "TRANSFER = 'transfer-learning'\n",
    "FINETUNE = 'fine-tuning'\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# CUDA\n",
    "train_on_gpu = cuda.is_available()\n",
    "print(f'[INFO] Train on gpu ...{train_on_gpu}')\n",
    "if train_on_gpu:\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f'[INFO] {gpu_count} gpus detected.')\n",
    "    if gpu_count > 1:\n",
    "        multi_gpu = True\n",
    "    else:\n",
    "        multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_path, img_size, transforms=None, phase=TRAIN):\n",
    "        self.data_path = Path(data_path / phase)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms[phase]\n",
    "        self.phase = phase\n",
    "\n",
    "        self.classes = self._get_classes()\n",
    "        self.image_paths = self._get_image_paths()\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.class_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "        self.idx_class = {idx: class_name for class_name, idx in self.class_idx.items()}\n",
    "\n",
    "        \n",
    "    def _get_classes(self):\n",
    "        return [f.name for f in (self.data_path).iterdir() if f.is_dir()]\n",
    "    \n",
    "\n",
    "    def _get_image_paths(self):\n",
    "        paths = list(self.data_path.rglob('*.jpg'))\n",
    "        random.shuffle( paths )\n",
    "        return paths\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx])\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = Path(img_path).parent.name\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "    def show_samples(self):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "        for i in range(10):\n",
    "            ax = fig.add_subplot(1, 10, i + 1)\n",
    "            _, label = self.__getitem__(i)\n",
    "            img_cv2 = self.get_cv2_img(i)\n",
    "\n",
    "            ax.imshow(img_cv2, cmap='gray')\n",
    "            ax.set_title(label)\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def show_distribution(self):\n",
    "        labels_count = Counter([self.__getitem__(i)[1] for i in tqdm(range(len(self.image_paths)))])\n",
    "        sorted_counts = sorted(labels_count.items())\n",
    "        labels, counts = zip(*sorted_counts)\n",
    "\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        bars = plt.bar(labels, counts, color='skyblue')\n",
    "        plt.xlabel(f'{DATA}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Counts per Emotion Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count,\n",
    "                    ha='center', va='bottom', color='black', fontsize=8) \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def get_cv2_img(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        return cv2.imread(str(img_path))\n",
    "    \n",
    "\n",
    "    def idx_to_class(self, idx_list):\n",
    "        return [self.idx_class[idx] for idx in idx_list]\n",
    "\n",
    "\n",
    "    def class_to_idx(self, class_list):\n",
    "        return [self.class_idx[class_name] for class_name in class_list]\n",
    "    \n",
    "    \n",
    "    def print_info(self):\n",
    "        print(f\"[INFO] Total number of images ...{len(self)}\")\n",
    "        print(\"[INFO] Number of classes: \", self.num_classes)\n",
    "        print(\"[INFO] Classes: \", self.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    x: Dataset(CURRENT_PATH, img_size=224, transforms=data_transforms, phase=x) for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x : len(datasets[x]) for x in [TRAIN, VAL, TEST] \n",
    "}\n",
    "\n",
    "n_classes = datasets[TRAIN].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n",
      "Training batch 0/160\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 191\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransfer_trained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_base\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_wts)\n\u001b[1;32m    190\u001b[0m vgg16 \u001b[38;5;241m=\u001b[39m VGG(n_classes\u001b[38;5;241m=\u001b[39mn_classes, mode\u001b[38;5;241m=\u001b[39mFEATURES, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)  \n\u001b[0;32m--> 191\u001b[0m \u001b[43mvgg16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransfer_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m          \n",
      "Cell \u001b[0;32mIn[106], line 119\u001b[0m, in \u001b[0;36mVGG.transfer_learning\u001b[0;34m(self, dataloaders, num_epochs)\u001b[0m\n\u001b[1;32m    117\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m--> 119\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    121\u001b[0m optimizer_ft\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "class VGG(torch.nn.Module):\n",
    "    def __init__(self, n_classes, mode=FEATURES, pretrained=True, model=16):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.conv_base = None\n",
    "        self.mode = mode\n",
    "        self.n_classes = n_classes\n",
    "        self.transfer_trained_model = None\n",
    "        self.cuda = self._check_cuda()\n",
    "\n",
    "        if model == 16:\n",
    "            self.conv_base = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        elif model == 19:\n",
    "            self.conv_base = models.vgg19(weights='IMAGENET1K_V1')\n",
    "        else: \n",
    "            raise ValueError('Unsupported mode in VGG model')\n",
    "\n",
    "        # this mode removes the classifier and returns extracted features.\n",
    "        if self.mode == FEATURES:\n",
    "            for param in self.conv_base.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.conv_base.classifier = torch.nn.Identity()\n",
    "        \n",
    "        # this mode replaces the last layer of classifier-part of the vgg16 net with a custom classifier layer, which returns one of the class labels.\n",
    "        elif self.mode == TRANSFER:\n",
    "            for param in self.conv_base.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.conv_base.classifier[-1] = torch.nn.Linear(in_features=self.conv_base.classifier[-1].in_features, out_features=n_classes)\n",
    "\n",
    "        # this mode fine-tunes the classifier part and maybe also some other layers within the net??\n",
    "        elif self.mode == FINETUNE:\n",
    "            pass\n",
    "\n",
    "        else: \n",
    "            raise ValueError('Unsupported mode in VGG16 / VGG19 init')\n",
    "    \n",
    "    def _check_cuda(self):\n",
    "            return torch.cuda.is_available()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def summary(self):\n",
    "        summary(self.conv_base, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')\n",
    "\n",
    "\n",
    "    def feature_extract(self, dataloaders):\n",
    "\n",
    "        features = np.empty((0, 25088))\n",
    "        labels = np.empty(0)\n",
    "\n",
    "        for phase in ([TRAIN, TEST, VAL]):\n",
    "\n",
    "            for inputs_batch, labels_batch in tqdm(dataloaders[phase]):\n",
    "                with torch.no_grad():\n",
    "                    features_batch = np.asarray(self.conv_base(inputs_batch))\n",
    "                    features = np.append(features, features_batch, axis=0)\n",
    "                    label = np.asarray((labels_batch)).flatten()\n",
    "\n",
    "                labels = np.append(labels, label)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "\n",
    "    def transfer_learning(self, dataloaders, num_epochs=10):\n",
    "\n",
    "        for param in self.conv_base.features.parameters():\n",
    "            param.require_grad = True\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_ft = optim.SGD(self.conv_base.parameters(), lr=0.001, momentum=0.9)\n",
    "        #exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "        since = time.time()\n",
    "        best_model_wts = copy.deepcopy(self.conv_base.state_dict())\n",
    "\n",
    "        best_acc = 0.0\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "        avg_loss_val = 0\n",
    "        avg_acc_val = 0\n",
    "\n",
    "        train_batches = len(dataloaders[TRAIN])\n",
    "        val_batches = len(dataloaders[VAL])\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "            print('-' * 10)\n",
    "\n",
    "            loss_train = 0\n",
    "            loss_val = 0\n",
    "            acc_train = 0\n",
    "            acc_val = 0\n",
    "            \n",
    "            self.conv_base.train(True)\n",
    "\n",
    "            for i, data in enumerate(dataloaders[TRAIN]):\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"\\rTraining batch {i}/{train_batches}\", flush=True)\n",
    "\n",
    "                if i >= train_batches / 2:\n",
    "                    break\n",
    "                \n",
    "                inputs = data[0]\n",
    "                labels = torch.tensor(np.asarray(dataloaders[TRAIN].dataset.class_to_idx(data[1])))\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                else:\n",
    "                    inputs, labels = inputs, labels\n",
    "\n",
    "                optimizer_ft.zero_grad()\n",
    "                outputs = self.conv_base(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return\n",
    "                optimizer_ft.step()\n",
    "                return\n",
    "                \n",
    "                \n",
    "                loss_train += loss.data[0]\n",
    "                acc_train += torch.sum(preds == labels.data)\n",
    "                \n",
    "                del inputs, labels, outputs, preds\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            print()\n",
    "            # * 2 as we only used half of the dataset\n",
    "            avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n",
    "            avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n",
    "            \n",
    "            self.conv_base.train(False)\n",
    "            self.conv_base.eval()\n",
    "\n",
    "            for i, data in enumerate(dataloaders[VAL]):\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"\\rValidation batch {i}/{val_batches}\", flush=True)\n",
    "                \n",
    "                inputs, labels = data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                else:\n",
    "                    inputs, labels = inputs, labels\n",
    "                \n",
    "                optimizer_ft.zero_grad()\n",
    "                \n",
    "                outputs = self.conv_base(inputs)\n",
    "                \n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss_val += loss.data[0]\n",
    "                acc_val += torch.sum(preds == labels.data)\n",
    "                \n",
    "                del inputs, labels, outputs, preds\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            avg_loss_val = loss_val / dataset_sizes[VAL]\n",
    "            avg_acc_val = acc_val / dataset_sizes[VAL]\n",
    "\n",
    "            print()\n",
    "            print(f\"Epoch {epoch} result: \".format(epoch))\n",
    "            print(f\"Avg loss (train): {avg_loss:.4f}\")\n",
    "            print(f\"Avg acc (train): {avg_acc:.4f}\")\n",
    "            print(f\"Avg loss (val): {avg_loss_val:.4f}\")\n",
    "            print(f\"Avg acc (val): {avg_acc_val:.4f}\")\n",
    "            print('-' * 10)\n",
    "            print()\n",
    "\n",
    "            if avg_acc_val > best_acc:\n",
    "                best_acc = avg_acc_val\n",
    "                best_model_wts = copy.deepcopy(self.conv_base.state_dict())\n",
    "\n",
    "        elapsed_time = time.time() - since\n",
    "\n",
    "        print()\n",
    "        print(f\"Training completed in {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "        print(f\"Best acc: {best_acc:.4f}\")\n",
    "\n",
    "        self.transfer_trained_model = self.conv_base.load_state_dict(best_model_wts)\n",
    "\n",
    "\n",
    "\n",
    "vgg16 = VGG(n_classes=n_classes, mode=FEATURES, pretrained=True, model=16)  \n",
    "vgg16.transfer_learning(dataloaders=dataloaders, num_epochs=10)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    \n",
    "        \n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    vgg.load_state_dict(best_model_wts)\n",
    "    return vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.transfer_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, n_classes, mode=FEATURES, pretrained=True):\n",
    "        super(Resnet, self).__init__()\n",
    "\n",
    "        self.conv_base = models.resnet50(weights='IMAGENET1K_V2')\n",
    "        self.mode = mode\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        if self.mode == FEATURES:\n",
    "            for param in self.conv_base.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.conv_base.fc = torch.nn.Identity()\n",
    "\n",
    "        elif self.mode == TRANSFER:\n",
    "            for param in self.conv_base.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.conv_base.fc = torch.nn.Linear(in_features=self.conv_base.classifier[-1].in_features, out_features=n_classes)\n",
    "\n",
    "        elif self.mode == FINETUNE:\n",
    "            pass\n",
    "        \n",
    "        else: \n",
    "            raise ValueError('Unsupported mode in Resnet init')\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def summary(self):\n",
    "        summary(self.conv_base, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')\n",
    "\n",
    "    \n",
    "    def feature_extract(self, dataloaders):\n",
    "\n",
    "        features = np.empty((0, 2048))\n",
    "        labels = np.empty(0)\n",
    "\n",
    "        for phase in ([TRAIN, TEST, VAL]):\n",
    "\n",
    "            for inputs_batch, labels_batch in tqdm(dataloaders[phase]):\n",
    "                with torch.no_grad():\n",
    "                    features_batch = np.asarray(self.conv_base(inputs_batch))\n",
    "                    features = np.append(features, features_batch, axis=0)\n",
    "                    label = np.asarray((labels_batch)).flatten()\n",
    "\n",
    "                labels = np.append(labels, label)\n",
    "    \n",
    "        return features, labels\n",
    "    \n",
    "    def simple_classify(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = Resnet(n_classes=n_classes, mode=FEATURES, pretrained=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'class_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nimhchefs \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCURRENT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_path, img_size, transforms, phase)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_image_paths()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_idx \u001b[38;5;241m=\u001b[39m {class_name: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, class_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)}\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx_class \u001b[38;5;241m=\u001b[39m {idx: class_name \u001b[38;5;28;01mfor\u001b[39;00m class_name, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'class_idx'"
     ]
    }
   ],
   "source": [
    "nimhchefs = Dataset(data_path=CURRENT_PATH, img_size=64, transforms=data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = resnet50.extract_features(dataloaders=dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = vgg16.extract_features(dataloaders=dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" self.linear1 = torch.nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
    "self.relu1 = torch.nn.ReLU(inplace=True)\n",
    "self.dropout1 = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "self.linear2 = torch.nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "self.relu2 = torch.nn.ReLU(inplace=True)\n",
    "self.dropout2 = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "self.linear3 = torch.nn.Linear(in_features=4096, out_features=n_classes, bias=True) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "for param in conv_base.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloaders):\n",
    "\n",
    "    features = torch.zeros(0, 5)\n",
    "    labels = torch.zeros(0, dtype=torch.long)\n",
    "    i = 0\n",
    "\n",
    "    for phase in ([TRAIN, TEST, VAL]):\n",
    "\n",
    "        \n",
    "        for inputs_batch, labels_batch in tqdm(dataloaders[phase]):\n",
    "            i += 1\n",
    "            with torch.no_grad():\n",
    "                features_batch = conv_base(inputs_batch)\n",
    "                features = torch.tensor((features, features_batch))\n",
    "\n",
    "            labels = torch.tensor((labels, labels_batch))\n",
    "            return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features(dataloaders=dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = image_datasets[TRAIN].class_to_idx\n",
    "idx_to_class = { idx: class_ for class_, idx in class_to_idx.items() }\n",
    "\n",
    "print(class_to_idx)\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = {}\n",
    "for value in labels:\n",
    "    category_counts[class_names[value.item()]] = category_counts.get(class_names[value.item()], 0) + 1\n",
    "\n",
    "sorted_counts = sorted(category_counts.items(), key=lambda x: x[0])\n",
    "categories, counts = zip(*sorted_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "bars = plt.bar(categories, counts, color='skyblue')\n",
    "plt.xlabel('Emotion Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Counts per Emotion Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, count, ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Number of images used in training ...\", x_train.shape[0])\n",
    "print(\"[INFO] Number of images used in testing ...\", x_test.shape[0])\n",
    "\n",
    "classifier = SVC()\n",
    "parameters = {'gamma': [0.1, 0.01, 0.001], 'C': [1, 10, 100, 1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(classifier, parameters, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"[INFO] Best params ...\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(clf, x_train, y_train, x_test, y_test, train=True):\n",
    "    if train:\n",
    "        y_prediction = clf.predict(x_train)\n",
    "        clf_report = classification_report(y_train, y_prediction)\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, y_prediction)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        y_prediction = clf.predict(x_test)\n",
    "        clf_report = classification_report(y_test, y_prediction)\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, y_prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, y_prediction)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(best_estimator, open('/home/jovyan/work/model.p', 'wb'))\n",
    "\n",
    "print_score(best_estimator, x_train, y_train, x_test, y_test, train=True)\n",
    "print_score(best_estimator, x_train, y_train, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_values = [3, 5, 10]\n",
    "\n",
    "for n_splits in n_splits_values:\n",
    "    cv = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "    scores = cross_val_score(best_estimator, features, labels, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    print(f\"{n_splits}-Fold CV: {scores.mean():.2f} accuracy with a standard deviation of {scores.std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
